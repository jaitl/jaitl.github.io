---
title: "Databases CAP"
author: ""
type: ""
date: 2022-01-15T12:00:00+03:00
subtitle: ""
image: ""
tags: []
private: false
---
Ещё одна статья про CAP-теорему. Я прочитал около 10 статей на тему CAP-теоремы и хочу обобщить полученную информацию.
<!--more-->

Перед тем как перейти непосредственно к разбору CAP-теоремы, нужно разобраться в моделях согласованности данных и типах репликации данных.

## Модели согласованности данных
Модель согласованности описывает то, как видят изменения данных разные части системы.
Например, есть некий кластер из N нод, в котором есть таблица, и в этой таблице хранится переменная `A=0`.
Так же, есть три сервиса, которые работают с этим кластером.
Допустим, первый сервис изменил эту переменную на `A=5`, затем второй сервис изменил ее на `A=10`, 
и после этого третий сервис считывает значение переменной `A`. 
В зависимости от используемой модели согласованности данных третий сервис может считать любое из трех значений: `0`, `5`, `10`.

Существует несколько моделей согласованности данных, если вам интересна эта тема, то обратите внимание на ссылки в конце статьи.
Тут я разберу только два обобщенных вида согласованности, которые необходимы для понимания темы.

### Строгая (Strong consistency)
Строгая модель предполагает, что сразу же после изменения/добавления данных в кластере, 
все сервисы работающие с этим кластером должны видеть новые/изменившиеся данные.

В примере выше, третий сервис всегда будет видеть свежие данные.
После всех изменений переменной `A`, он обязательно считает последнее значение равное `10`.

### Слабая (Weak Consistency)
Слабая модель предполагает, что данные после изменения/добавления в кластер могут обновляться на всех нодах кластера в течении какого-то периода времени.

В примере выше, третий сервис может считать любое из трех значений переменной `A`: `0`, `5`, `10`.

## Типы репликации данных
Репликация - это синхронизация данных между несколькими нодами базы данных в кластере.

Например, есть три ноды с БД и мы хотим что бы на всех трех нодах были одни и те же данные. 
Это может потребоваться для:
1. Бекапа, если что то случится с одной из нод, то у нас будет еще две копии этих данных.
2. Отказоустойчивости, то же самое что и в пункте 1, если одна из нод выйдет из стоя, то останется еще две ноды.
3. Распределения нагрузки, если у нас большое количество запросов на чтение или на запись, то запросы можно равномерно распределить между тремя нодами.

Я опишу два основных вида репликации, встречаются различные вариации этих двух видов, все зависит от возможностей конкретной базы данных.

### master-slave
В случае `master-slave` репликации, есть одна главная нода - `master`, которая обрабатывает запросы на запись и чтение данных. 
К главной ноде подключены еще N дополнительных нод-реплик - `slave`, которые являются полными копиями `master` ноды. 
`slave`-реплики обрабатывают только запросы на чтение, в `slave` нельзя записать данные. 
Когда новые данные добавляются в `master`-ноду, эти данные отправляются во все `slave`-реплики.

Если `master` нода выходит из строя, то выбирается `slave`-нода и становится новой `master`-нодой. 
После чего, все запросы на запись идут в новую `master`-ноду. 
Иногда процесс выбора новой главной ноды выполняется вручную, через изменение конфигурации, 
а иногда автоматически, это зависит от уровня автоматизации и уверенности админов.

### multi-master (одноранговая, master-master, P2P)
В случае `multi-master` репликации все ноды являются `master`-нодами и все они главные.
Каждая нода умеет обрабатывать запросы на запись и на чтение. 
После записи данных на одну `master`-ноду, данные реплицируются на остальные `master`-ноды.

Если из стоя выйдет одна `master`-нода, то оставшиеся `master`-ноды продолжают работать, реконфигурация не требуется.

### Синхронная и Асинхронная репликация данных
Данные между нодами могут реплицироваться двумя способами - синхронно и асинхронно.

Синхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master`-нодой и присылает ей новые данные.
2. `master`-нода сохраняет новые данные к себе и синхронно отправляет эти данные в другие ноды.
3. Все остальные ноды сохраняют новые данные к себе и говорят `master`-ноде, что сохранение прошло успешно.
4. `master`-нода, сообщает клиенту что сохранение новых данных прошло успешно и разрывает соединение с клиентом.

В случае синхронной репликации, запрос на сохранение данных для клиента занимает достаточно долгое время, 
потому что клиент должен дождаться, когда все ноды сохранят новые данные, только после этого клиент получит ответ, что сохранение прошло успешно.

Асинхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master`-нодой и присылает ей новые данные.
2. `master`-нода сохраняет новые данные к себе, затем сообщает клиенту, что сохранение новых данных прошло успешно и разрывает соединение с клиентом.
3. Затем в фоне, все остальные ноды забирают у `master`-ноды новые данные и сохраняют их к себе.

В случае асинхронной репликации, запрос на сохранение данных для клиента занимает меньше времени, клиент не ждет сохранения данных на все ноды.
Так как репликация данных между всеми нодами происходит в фоне, эта операция занимает время, в результате появляется риск,
что от какого-то клиента может прийти запрос на чтение новых данных на другую ноду, не на ту куда выполнялось сохранение этих данных 
и клиент не получит новые данные.

При асинхронной репликации есть риск потери данных, например, когда новые данные были сохранены на `master`-ноду, 
а затем сразу же после успешного сохранения эта нода сразу же выходит из строя.
В этом случае остальные ноды не смогут забрать себе новые данные и эти данные будут потеряны.

Теперь можно провести параллель с моделями согласованности данных и увидеть, что синхронная репликация относится к строгой модели согласованности данных,
а асинхронная репликация относится к слабой модели согласованности данных.

### Кворум
Кворум используется при репликации `multi-master`, когда:
* необходимо повысить надежность сохранения данных
* необходимо получить строгую модель согласованности данных

Кворум - это режим записи данных сразу на несколько `master`-нод и чтения данных, сразу с нескольких `master`-нод.
Количество нод входящих в кворум определяется по формуле `(n/2 + 1)`. 
Например, если у нас `5` нод, то в режиме кворума, запись и чтение будут выполняться на `3` ноды сразу.

Кворум позволяет повысить надежность записи данных, потому что данные будут записаны сразу на несколько нод, 
а вероятность выхода из строя сразу нескольких нод минимальна.
Но иногда, из строя может выйти целый дата-центр. 
Если требуется очень большая устойчивость к сбоям, но разные ноды одной БД располагают сразу в нескольких дата центрах.
В случае наличия нескольких дата центов, данные сохраняются сразу в несколько `master`-нод в разных дата центрах.

Если использовать сохранения данных в режиме кворума и чтение в режиме кворума, то запрос на чтение будет отправлен сразу на несколько нод,
результаты чтения будут отсортированы по версиям и данные с самой последней версией будут отданы клиенту.
Это гарантирует, что клиент всегда получит самую последнюю версию данных и позволяет добиться строгой модели согласованности данных.

## CAP
CAP теорема описывает компромиссы при разработке и использовании распределенных систем хранения данных с репликацией.
К таким системам относятся базы данных, распределенные файловые системы.

Три буквы в слове CAP, являются свойствами распределенной системы: Consistency, Availability, Partition tolerance.
Теорема гласит, что возможно выбрать только два свойства из трех. Следовательно, необходимо пожертвовать одним из свойств.
Каким свойством жертвовать, зависит от требований к конкретной распределенной системе.

Разберем, что означает каждое свойство из CAP, используя знания полученные выше.

### Consistency (Согласованность)
Под согласованностью в CAP теореме понимается строгая модель согласованности данных. 
Следовательно, каждый сервис всегда должен видеть самую последнюю и свежую версию данных.

Как я уже писал выше, строгой модели согласованности можно добиться двумя путями:
1. Использовать синхронную репликацию.
2. Использовать асинхронную репликацию с записью и чтением данных в режиме кворума.

### Availability (Доступность)
Под доступностью в CAP теореме понимается, что если нода в рабочем состоянии, она должна успешно отвечать на любой запрос чтения/записи данных.

Одним из недостатков свойства доступности в CAP, является то, что CAP не учитывается лейтенси.
Например, если нода отвечала на запрос 10 минут, то с точки зрения доступности в CAP это нормально и нода считается доступной.
В реальных системах такое большое лейтенси является проблемой. 
По этому можно сделать допущение и считать, что нода считается доступной, если она отвечает на запрос за разумный период времени, например за 200мс.
Это допущение противоречит оригинальной CAP теореме, но большинство делает его, потому что оно ближе к действительности.

### Partition tolerance (Устойчивость к разделению)
Под устойчивостью к разделению понимаются проблемы с сетью.
Это могут быть задержки, потери пакетов или полный разрыв соединения между нодами, который приводит к тому, что ноды не видят друг друга.

Распределенная система, состоящая из нескольких нод подразумевает, что она имеет дело с сетью.
Следовательно, нельзя пожертвовать этим свойством CAP теоремы, 
потому что без устойчивости к сетевым проблемам любая распределенная система будет работать нестабильно.

## Комбинации
CAP теорема гласит, что возможно выбрать только два из трех выше описанных свойств.
В результате получаются три комбинации: CP, AP, CA.

Разберем основные свойства каждой из этих трех комбинаций.

### CP: Выбираем Consistency и Partition tolerance, жертвуем Availability
Распределенная система устойчива к сетевым проблемам и соответствует строгой модели согласованности,
но при проблемах с сетью часть системы становится недоступной.

Под эту категорию подходит `master-slave` репликация, один из `slave` может стать недоступным для чтения, но `master` продолжит принимать запросы на записи и чтение.

Еще можно привести пример с двумя дата центрами. 
Допустим есть два дата центра в разных регионах страны. В какой то момент соединение между ними теряется.
Второй дата центр должен перестать принимать запросы на чтение и запись, а приложения работавшие с этим дата центром, должны переключиться на первый рабочий дата центр.

### AP: Выбираем Availability и Partition tolerance, жертвуем Consistency
Распределенная система устойчива к сетевым проблемам и всегда доступна, но использует слабую модель согласованности. 
При сетевых проблемах разные ноды могут выдавать разные данные на один и тот же запрос, потому что они работают независимо и не имеют связи друг с другом.

Под это определение подходит `master-master` конфигурация, в которой каждая нода может независимо обрабатывать запросы на запись и чтение и при потере связи, репликация останосится, но ноды продолжат работать независимо, хоть и не будут знать об новых данных друг у друга.

Так же возможна и `master-slave` конфигурация, при потере связанности `slave` продолжит выполнять запросы на чтение, но он будет отдавать только старые данные, потому что о новых он ничего не знает.

В примере с двумя дата центрами, когда между ними происходит разрыв соединения, оба дата центра становятся на время независимыми и не видят обновления друг у друга, в результате чего часть данных в них устаревает до момента возобновления соединения.

### CA: Выбираем Consistency и Availability, жертвуем Partition tolerance
Как я уже писал выше в распределенной системе, нельзя пожертвовать `Partition tolerance`, потому что распределенная система всегда имеет дело с сетью. 
К этой категории можно отнести системы хранения данных у которых несколько агентов установлены на один физический сервер и не подвержены проблемам с сетью.

Например, на один мощный сервер можно поставить две копии БД в режиме синхронной `master-slave` репликации.
Но это очень странная конфигурация, не думаю что кто-то будет такое делать в реальной прадакреш системе.

## Типы баз данных
Сложно отнести современные БД к одной категории. Например для Cassandra можно настроить
MySQL может быть CP или AP в зависимости от конфигурации.


## Недостатки CAP-теоремы

## Дальнейшее чтение
* [Overview of Consistency Levels in Database Systems](http://dbmsmusings.blogspot.com/2019/07/overview-of-consistency-levels-in.html)
* [Consistency model Wikipedia](https://en.wikipedia.org/wiki/Consistency_model)
* [Big Data World, Part 5: CAP Theorem](https://blog.jetbrains.com/blog/2021/06/03/big-data-world-part-5-cap-theorem/)
* [Мифы о CAP теореме](https://habr.com/ru/post/322276/)
* [Please stop calling databases CP or AP](https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html) ([Перевод на русский](https://habr.com/ru/post/258145/))
