---
title: "Databases CAP"
author: ""
type: ""
date: 2022-01-15T12:00:00+03:00
subtitle: ""
image: ""
tags: []
private: false
---
Ещё одна статья про CAP-теорему. Я прочитал около 10 статей на тему CAP-теоремы и хочу обобщить полученную информацию.
В статье я рассматриваю модели согласованности данных, типы репликации данных, свойства CAP теоремы, соотношу CAP теорему и типы баз данных.
В конце я подвожу итог, в котором объясняю почему CAP теорема является формальным и неточным описанием распределенных систем и почему на нее не стоит рассчитывать.
<!--more-->

Перед тем как перейти непосредственно к разбору CAP-теоремы, нужно разобраться в моделях согласованности данных и типах репликации данных.

## Модели согласованности данных
Модель согласованности описывает то, как видят изменения данных разные части системы.
Например, есть некий кластер из N нод, в котором есть таблица, и в этой таблице хранится переменная `A=0`.
Так же, есть три сервиса, которые работают с этим кластером.
Допустим, первый сервис изменил эту переменную на `A=5`, затем второй сервис изменил ее на `A=10`, 
и после этого третий сервис считывает значение переменной `A`. 
В зависимости от используемой модели согласованности данных третий сервис может считать любое из трех значений: `0`, `5`, `10`.

Существует несколько моделей согласованности данных, если вам интересна эта тема, то обратите внимание на ссылки в конце статьи.
Тут я разберу только два обобщенных вида согласованности, которые необходимы для понимания темы.

### Строгая (Strong consistency)
Строгая модель предполагает, что сразу же после изменения/добавления данных в кластере, 
все сервисы работающие с этим кластером должны видеть новые/изменившиеся данные.

В примере выше, третий сервис всегда будет видеть свежие данные.
После всех изменений переменной `A`, он обязательно считает последнее значение равное `10`.

### Слабая (Weak Consistency)
Слабая модель предполагает, что данные после изменения/добавления в кластер могут обновляться на всех нодах кластера в течении какого-то периода времени.

В примере выше, третий сервис может считать любое из трех значений переменной `A`: `0`, `5`, `10`.

## Типы репликации данных
Репликация - это синхронизация данных между несколькими нодами базы данных в кластере.

Например, есть три ноды с БД и мы хотим что бы на всех трех нодах были одни и те же данные. 
Это может потребоваться для:
1. Бекапа, если что то случится с одной из нод, то у нас будет еще две копии этих данных.
2. Отказоустойчивости, то же самое что и в пункте 1, если одна из нод выйдет из стоя, то останется еще две ноды.
3. Распределения нагрузки, если у нас большое количество запросов на чтение или на запись, то запросы можно равномерно распределить между тремя нодами.

Я опишу два основных вида репликации, встречаются различные вариации этих двух видов, все зависит от возможностей конкретной базы данных.

### master-slave
В случае `master-slave` репликации, есть одна главная нода - `master`, которая обрабатывает запросы на запись и чтение данных. 
К главной ноде подключены еще N дополнительных нод-реплик - `slave`, которые являются полными копиями `master` ноды. 
`slave`-реплики обрабатывают только запросы на чтение, в `slave` нельзя записать данные. 
Когда новые данные добавляются в `master`-ноду, эти данные отправляются во все `slave`-реплики.

Если `master` нода выходит из строя, то выбирается `slave`-нода и становится новой `master`-нодой. 
После чего, все запросы на запись идут в новую `master`-ноду. 
Иногда процесс выбора новой главной ноды выполняется вручную, через изменение конфигурации, 
а иногда автоматически, это зависит от уровня автоматизации и уверенности админов.

### multi-master (одноранговая, master-master, P2P)
В случае `multi-master` репликации все ноды являются `master`-нодами и все они главные.
Каждая нода умеет обрабатывать запросы на запись и на чтение. 
После записи данных на одну `master`-ноду, данные реплицируются на остальные `master`-ноды.

Если из стоя выйдет одна `master`-нода, то оставшиеся `master`-ноды продолжают работать, реконфигурация не требуется.

### Синхронная и Асинхронная репликация данных
Данные между нодами могут реплицироваться двумя способами - синхронно и асинхронно.

Синхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master`-нодой и присылает ей новые данные.
2. `master`-нода сохраняет новые данные к себе и синхронно отправляет эти данные в другие ноды.
3. Все остальные ноды сохраняют новые данные к себе и говорят `master`-ноде, что сохранение прошло успешно.
4. `master`-нода, сообщает клиенту что сохранение новых данных прошло успешно и разрывает соединение с клиентом.

В случае синхронной репликации, запрос на сохранение данных для клиента занимает достаточно долгое время, 
потому что клиент должен дождаться, когда все ноды сохранят новые данные, только после этого клиент получит ответ, что сохранение прошло успешно.

Асинхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master`-нодой и присылает ей новые данные.
2. `master`-нода сохраняет новые данные к себе, затем сообщает клиенту, что сохранение новых данных прошло успешно и разрывает соединение с клиентом.
3. Затем в фоне, все остальные ноды забирают у `master`-ноды новые данные и сохраняют их к себе.

В случае асинхронной репликации, запрос на сохранение данных для клиента занимает меньше времени, клиент не ждет сохранения данных на все ноды.
Так как репликация данных между всеми нодами происходит в фоне, эта операция занимает время, в результате появляется риск,
что от какого-то клиента может прийти запрос на чтение новых данных на другую ноду, не на ту куда выполнялось сохранение этих данных 
и клиент не получит новые данные.

При асинхронной репликации есть риск потери данных, например, когда новые данные были сохранены на `master`-ноду, 
а затем сразу же после успешного сохранения эта нода сразу же выходит из строя.
В этом случае остальные ноды не смогут забрать себе новые данные и эти данные будут потеряны.

Теперь можно провести параллель с моделями согласованности данных и увидеть, что синхронная репликация относится к строгой модели согласованности данных,
а асинхронная репликация относится к слабой модели согласованности данных.

### Кворум
Кворум используется при репликации `multi-master`, когда:
* необходимо повысить надежность сохранения данных
* необходимо получить строгую модель согласованности данных

Кворум - это режим записи данных сразу на несколько `master`-нод и чтения данных, сразу с нескольких `master`-нод.
Количество нод входящих в кворум определяется по формуле `(n/2 + 1)`. 
Например, если у нас `5` нод, то в режиме кворума, запись и чтение будут выполняться на `3` ноды сразу.

Кворум позволяет повысить надежность записи данных, потому что данные будут записаны сразу на несколько нод, 
а вероятность выхода из строя сразу нескольких нод минимальна.
Но иногда, из строя может выйти целый дата-центр. 
Если требуется очень большая устойчивость к сбоям, но разные ноды одной БД располагают сразу в нескольких дата центрах.
В случае наличия нескольких дата центов, данные сохраняются сразу в несколько `master`-нод в разных дата центрах.

Если использовать сохранения данных в режиме кворума и чтение в режиме кворума, то запрос на чтение будет отправлен сразу на несколько нод,
результаты чтения будут отсортированы по версиям и данные с самой последней версией будут отданы клиенту.
Это гарантирует, что клиент всегда получит самую последнюю версию данных и позволяет добиться строгой модели согласованности данных.

## CAP
CAP теорема описывает компромиссы при разработке и использовании распределенных систем хранения данных с репликацией.
К таким системам относятся базы данных, распределенные файловые системы.

Три буквы в слове CAP, являются свойствами распределенной системы: Consistency, Availability, Partition tolerance.
Теорема гласит, что возможно выбрать только два свойства из трех. Следовательно, необходимо пожертвовать одним из свойств.
Каким свойством жертвовать, зависит от требований к конкретной распределенной системе.

Разберем, что означает каждое свойство из CAP, используя знания полученные выше.

### Consistency (Согласованность)
Под согласованностью в CAP теореме понимается строгая модель согласованности данных. 
Следовательно, каждый сервис всегда должен видеть самую последнюю и свежую версию данных.

Как я уже писал выше, строгой модели согласованности можно добиться двумя путями:
1. Использовать синхронную репликацию.
2. Использовать асинхронную репликацию с записью и чтением данных в режиме кворума.

### Availability (Доступность)
Под доступностью в CAP теореме понимается, что если нода в рабочем состоянии, она должна успешно отвечать на любой запрос чтения/записи данных.

Одним из недостатков свойства доступности в CAP, является то, что CAP не учитывается лейтенси.
Например, если нода отвечала на запрос 10 минут, то с точки зрения доступности в CAP это нормально и нода считается доступной.
В реальных системах такое большое лейтенси является проблемой. 
По этому можно сделать допущение и считать, что нода считается доступной, если она отвечает на запрос за разумный период времени, например за 200мс.
Это допущение противоречит оригинальной CAP теореме, но большинство делает его, потому что оно ближе к действительности.

### Partition tolerance (Устойчивость к разделению)
Под устойчивостью к разделению понимаются проблемы с сетью.
Это могут быть задержки, потери пакетов или полный разрыв соединения между нодами, который приводит к тому, что ноды не видят друг друга (сплит брейн).

Распределенная система, состоящая из нескольких нод подразумевает, что она имеет дело с сетью.
Следовательно, нельзя пожертвовать этим свойством CAP теоремы, 
потому что без устойчивости к сетевым проблемам любая распределенная система будет работать нестабильно.

## Комбинации
CAP теорема гласит, что возможно выбрать только два из трех выше описанных свойств.
В результате получаются три комбинации: CP, AP, CA.

Разберем основные свойства каждой из этих трех комбинаций.

### CP: Выбираем Consistency и Partition tolerance, жертвуем Availability
Распределенная система устойчива к сетевым проблемам и соответствует строгой модели согласованности,
но при проблемах с сетью часть системы становится недоступной.

Например, при `master-slave` репликации, одна из `slave`-нод может стать недоступной для чтения,
но `master`-нода продолжит принимать запросы на записи и чтение.

Еще можно привести пример с двумя дата центрами. 
Допустим есть два дата центра в разных регионах страны. Если в какой то момент соединение между ними теряется,
то второй дата центр должен перестать принимать запросы на чтение и запись,
а приложения работавшие с этим дата центром, должны переключиться на первый рабочий дата центр.

### AP: Выбираем Availability и Partition tolerance, жертвуем Consistency
Распределенная система устойчива к сетевым проблемам и всегда доступна, но использует слабую модель согласованности. 
При сетевых проблемах разные ноды могут выдавать разные данные на один и тот же запрос,
потому что они работают независимо и репликация между ними не выполняется.

Например, при `multi-master` конфигурации,
в которой каждая нода может независимо обрабатывать запросы на запись/чтение и при потере связи репликация остановится,
но ноды продолжат работать независимо, хоть и не будут знать об новых данных друг у друга.

Или например при `master-slave` конфигурации, во время потери связанности `slave`-ноды могут продолжить выполнять запросы на чтение, 
но они будут отдавать только старые данные, потому что репликация не выполняется.

В примере с двумя дата центрами, когда между ними происходит разрыв соединения, оба дата центра становятся на время независимыми и 
не видят обновления друг у друга, в результате чего часть данных в них устаревает до момента возобновления соединения.

### CA: Выбираем Consistency и Availability, жертвуем Partition tolerance
Как я уже писал выше в распределенной системе нельзя пожертвовать `Partition tolerance`, потому что распределенная система всегда имеет дело с сетью.
К этой категории возможно "с натяжкой" отнести базы данных работающие в режиме одной нодой без репликации данных.

## Типы баз данных
Рассмотрим популярные типы баз данных с точки зрения CAP теоремы.

### Реляционные базы данных (Relational Database, RDBM)
Реляционные БД один из самых старших и зрелых видов БД и они являются одними их самых гибких в настройке.
Большинство из популярных баз поддерживают как `master-slave`, так и `multi-master` репликацию.
Репликация может быть и синхронной и асинхронной.
Базу возможно сконфигурировать как со строгой согласованностью данных так и со слабой.

Реляционные БД могут попадать в категорию CP, либо AP по CAP в зависимости от типа репликации данных и настроек.

### Документные базы данных (Document-oriented database)
Типичный представитель этого типа баз данных - MongoDB.
MongoDB поддерживает только асинхронную `master-slave` репликацию данных.
По умолчанию чтение и запись выполняется через `master`-ноду. 
В конфигурации по умолчанию БД поддерживает строгую согласованность данных и является CP по CAP.
Но при падении `master`-ноды, будет выбрана новая `master`-нода и на новой ноде может не оказаться новых данных из-за асинхронной репликации.
Поэтому в момент автоматической смены `master`-ноды БД гарантирует только слабую согласованность.

В MongoDB возможно сконфигурировать чтение с `slave`-нод, так же есть настройки уровня согласованности данных при репликации между `master` и `slave`.
Благодаря этим настройкам возможно добиться нужного уровня согласованности данных при чтении с `slave`-нод и достичь либо CP, либо AP по CAP.

Делаем вывод, что в зависимости от конфигурации MongoDB следует рассматривать как CP, либо как AP по CAP.

### Column-family databases
Прародитель этого типа баз данных - Google BigTable, а точнее [эта статья об BigTable](https://static.googleusercontent.com/media/research.google.com/ru//archive/bigtable-osdi06.pdf).

Наиболее популярным open source представителем этой категории баз данных является Cassandra, о ней и поговорим.
Cassandra с самого начала создавалась как распределенная база данных для высоконагруженных систем с упором на масштабируемость.

Эта БД поддерживает только асинхронную `multi-master` репликацию данных. 
Для каждого запроса на чтение и запись, можно задать параметр [уровень согласованности](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html), 
и таким образом управлять уровнем согласованности данных.
Если указать значение меньшее чем количество нод в кворуме, то получится слабая согласованность данных,
а если указать кворум, то получится строгая согласованность данных.

Из этого можно сделать вывод, что в зависимости от запроса эта БД может относится как к CP, так и к AP типу по CAP теореме.
Благодаря возможности задать уровень согласованности прямо в запросе, некоторые запросы могут быть CP типа, а некоторые AP типа.

## Компромиссы CAP теоремы
Большинство баз данных можно настроить как БД CP типа, либо как AP типа по CAP. 
Базы предоставляют гибкую конфигурацию модели согласованности данных.
Сложно отнести какую либо БД к одному конкретному типу по CAP.

CAP теорема обладает большим количеством недостатков, например:
* в Availability ничего не сказано про лейтенси
* под Consistency понимается только строгая модель согласованности данных
* теорема не учитывает транзакции

Подробнее про недостатки CAP теоремы можно почитать в статье [Please stop calling databases CP or AP](https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html) ([Перевод на русский](https://habr.com/ru/post/258145/)).

Не стоит считать CAP теорему истиной в последней инстанции.
К CAP следует относится как к формальной теореме, что бы понять какие компромиссы существуют в распределенных БД.
Из-за большого количества недостатков распределение БД по CAP очень условный и неточный подход, т.к.
БД имеют гибкие настройки согласованности данных, которые сложно учесть при соотнесении их с CAP теоремой.

CAP теорему можно использовать только при поверхностном описании БД, 
что бы слушатель/читатель мог примерно понять каких свойств по доступности и согласованности данных вы пытаетесь достичь.

Более точным подходом будет выбор между строгостью модели согласованности данных и получаемым в результате лейтенси. 
Чем выше согласованность данных, тем больше времени нужно на выполнение запросов на чтение/запись
и следовательно тем выше будет лейтенси. И наоборот, при слабой согласованности запросы выполняются быстрее и лейтенси уменьшается.
Потребуется рассмотреть конкретную предметную область и требования к системе, что бы найти компромисс между моделью согласованности и лейтенси.

## Дальнейшее чтение
* [NoSQL Distilled Book](https://www.amazon.com/gp/product/0321826620)
* [Overview of Consistency Levels in Database Systems](http://dbmsmusings.blogspot.com/2019/07/overview-of-consistency-levels-in.html)
* [Please stop calling databases CP or AP](https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html) ([Перевод на русский](https://habr.com/ru/post/258145/))
* [Consistency model Wikipedia](https://en.wikipedia.org/wiki/Consistency_model)
* [Big Data World, Part 5: CAP Theorem](https://blog.jetbrains.com/blog/2021/06/03/big-data-world-part-5-cap-theorem/)
* [Мифы о CAP теореме](https://habr.com/ru/post/322276/)
