---
title: "Databases CAP"
author: ""
type: ""
date: 2022-01-15T12:00:00+03:00
subtitle: ""
image: ""
tags: []
private: false
---
Ещё одна статья про CAP-теорему. Я прочитал около 10 статей на тему CAP-теоремы и хочу обобщить полученную информацию.
<!--more-->

Перед тем как перейти непосредственно к разбору CAP-теоремы, нужно разобраться в моделях согласованности данных и типах репликации данных.

## Модели согласованности данных
Модель согласованности описывает то, как видят изменения данных разные части системы.
Например, есть переменная `A=0`, первый сервис изменил эту переменную на `A=5`, второй сервис изменил ее на `A=10`, 
а третий сервис считывает значение переменной `A`. 
В зависимости от используемой модели согласованности данных третий сервис может считать любое из трех значений: `0`, `5`, `10`.

Существует несколько моделей согласованности данных, если вам интересна эта тема, то обратите внимание на ссылки в конце статьи.
Тут я разберу только два обобщенных вида согласованности, которые необходимы для понимания темы.

### Строгая (Strong consistency)
Строгая модель предполагает, что сразу же после изменения/добавления данных, все сервисы должны видеть новые/изменившиеся данные.

В примере выше, третий сервис всегда будет видеть свежие данные. 
После всех изменений переменной `A`, он обязательно считает последнее значение равное `10`.

### Слабая (Weak Consistency)
Слабая модель предполагает, что данные могут обновляться в течении какого-то периода времени после изменения/добавления данных.

В примере выше, третий сервис может считать любое из трех значений переменной `A`: `0`, `5`, `10`.

## Типы репликации данных
Репликация - это синхронизация данных между несколькими нодами базы данных.

Например, есть три ноды с БД и мы хотим что бы на всех трех нодах были одни и те же данные. 
Это может потребоваться для:
1. Бекапа, если что то случится с одной из нод, то у нас будет еще две копии этих данных.
2. Отказоустойчивости, то же самое что и в пункте 1, если одна из нод выйдет из стоя, то останется еще две ноды.
3. Распределения нагрузки, если у нас большое количество запросов на чтение или на запись, то запросы можно равномерно распределить между тремя нодами.

Я опишу два основных вида репликации, встречаются различные вариации этих двух видов, все зависит от возможностей конкретной базы данных.

### master-slave
В случае `master-slave` репликации, есть одна главная нода - `master` базы данных, которая обрабатывает запросы на запись и чтение данных. 
К главной ноде подключены еще N дополнительных нод-реплик - `slave`, которые являются копиями `master` ноды. 
`slave`-реплики обрабатывают только запросы на чтение, в `slave` нельзя записать данные. 
Когда новые данные добавляются в `master` ноду, эти данные отправляются во все `slave`-реплики.

Если `master` нода выходит из строя, то выбирается `slave` и становится новой `master` нодой. 
После чего, все запросы на запись идут в новую `master` ноду. 
Иногда процесс выбора новой главной ноды выполняется вручную, через изменение конфигурации, 
а иногда автоматически, это зависит от уровня автоматизации и уверенности админов.

### multi-master (одноранговая, master-master, P2P)
В случае `multi-master` репликации все ноды главные - `master`. Каждая нода умеет обрабатывать запросы на запись и на чтение. 
После записи данных на одну ноду, данные синхронизируются на остальные ноды.

Если из стоя выйдет одна `master` нода, то остальные ноды продолжают работать, реконфигурация не требуется.

### Синхронная и Асинхронная репликация данных
Данные между нодами могут синхронизироваться двумя способами - синхронно и асинхронно.

Синхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master` нодой и присылает новые данные.
2. `master` нода сохраняет новые данные к себе и синхронно отправляет эти данные в другие ноды.
3. Все остальные ноды сохраняют новые данные к себе и говорят `master` ноде, что сохранение прошло успешно.
4. `master` нода, сообщает клиенту что сохранение новых данных прошло успешно и разрывает соединение с клиентом.

В случае синхронной репликации, запрос на сохранение данных для клиента занимает достаточно долгое время, 
потому что клиент должен дождаться, когда все ноды сохранят новые данные, только после этого клиент получит ответ, что сохранение прошло успешно.

Асинхронная репликация выглядит так:
1. Клиент устанавливает соединение с `master` нодой и присылает новые данные.
2. `master` нода сохраняет новые данные к себе, затем сообщает клиенту, что сохранение новых данных прошло успешно и разрывает соединение с клиентом.
3. Затем в фоне, все остальные ноды забирают у `master` ноды новые данные и сохраняют их к себе.

В случае асинхронной репликации, запрос на сохранение данных для клиента занимает меньше времени, клиент не ждет сохранения данных на все ноды.
Так как репликация данных между всеми нодами происходит в фоне, эта операция занимает какое-то время, в результате появляется риск,
что от какого-то клиента может прийти запрос на чтение новых данных на другую ноду, не на ту куда выполнялось сохранение этих данных 
и клиент не получит новые данные.

При асинхронной репликации есть риск потери данных, например, когда новые данные были сохранены на `master` ноду, 
а затем сразу же после успешного сохранения эта нода сразу же выходит из строя.
В этом случае остальные ноды не смогут забрать себе новые данные и эти данные будут потеряны.

Тут можно провести параллель с моделями согласованности данных и увидеть, что синхронная репликация относится к строгой модели согласованности данных,
а асинхронная репликация относится к слабой модели согласованности данных.

### Кворум
Кворум используется при репликацию `multi-master` типа, когда:
* необходимо повысить надежность сохранения данных
* необходимо получить строгую модель согласованности данных

Кворум - это режим записи данных сразу на несколько `master` нод и чтения данных, сразу с нескольких `master` нод.
Количество нод входящих в кворум определяется по формуле `(n/2 + 1)`. 
Например, если у нас 5 нод, то в режиме кворума, запись и чтение будут выполняться на 3 ноды сразу.

Кворум позволяет повысить надежность записи данных, потому что данные будут записаны сразу на несколько нод, 
а вероятность выхода из строя сразу нескольких нод минимальна.
Но иногда, из строя может выйти целый дата-центр. 
Если требуется очень большая устойчивость к сбоям, но разные ноды одной БД располагают сразу в нескольких дата центрах.
В случае наличия нескольких дата центов, данные сохраняются сразу в несколько `master` нод в разных дата центрах.

Если после сохранения данных в режиме кворума, использовать чтение в режиме кворума, то запрос на чтение будет отправлен сразу на несколько нод,
результаты чтения будут отсортированы по версиям и данные с самой последней версией будут отданы клиенту.
Это гарантирует, что клиент всегда получит самую последнюю версию данных и позволяет добиться строгой модели согласованности данных.

## CAP
Теорема состоит из трех свойств.

### Consistency (Согласованность)
У CAP теоремы, строгое понимание согласованности, она понимает Sequential consistency или Linearizability. 
Каждая нода всегда должна отдавать самые последние данные.
Под это определение подходит только синхронная репликация между нодами. 
Асинхронная репликация гарантирует только Eventual Consistency, что не подходит под определение Consistency в CAP.

### Availability (Доступность)
По CAP, если нода не упала и она возвращает хоть какой то ответ, то нода считается доступной. 
Но в CAP не учитывается лейтенси, если нода отвечала 10 минут, но это ок и по CAP эта нода считается доступной.

### Partition tolerance (Устойчивость к разделению)
Система продолжает работать, даже если возникли сетевые проблемы, например две ноды не видят друг друга.

## Комбинации
Теорема гласит, что можно выбрать максимум два из трех выше описанных свойств.

### CP
Система согласована, каждый запрос всегда возвращает свежие данные. Но когда происходят сетевые проблемы, часть системы может стать недоступной. 

Под это определение подходит `master-slave` репликация, один из `slave` может стать недоступным для чтения, но `master` продолжит принимать запросы на записи и чтение.

Еще можно привести пример с двумя дата центрами. 
У нас есть два дата центра. В какой то момент соединение между ними теряется.
 Второй дата центр должен перестать принимать запросы на чтение и запись, а приложения работавшие с этим дата центром, должны переключиться на первый рабочий дата центр.

### AP
Система доступна, но при сетевых проблемах разные ноды могут выдавать разные данные, на один и тот же запрос, потому что они работают независимо. 
Но после восстановления сети, система придет в согласованное состояние. По сути мы получает Eventual Consistency систему. 

Под это определение подходит `master-master` конфигурация, в которой каждая нода может независимо обрабатывать запросы на запись и чтение и при потере связи, репликация останосится, но ноды продолжат работать независимо, хоть и не будут знать об новых данных друг у друга.

Так же возможна и `master-slave` конфигурация, при потере связанности `slave` продолжит выполнять запросы на чтение, но он будет отдавать только старые данные, потому что о новых он ничего не знает.

В примере с двумя дата центрами, когда между ними происходит разрыв соединения, оба дата центра становятся на время независимыми и не видят обновления друг у друга, в результате чего часть данных в них устаревает до момента возобновления соединения.

### CA
Так как CAP, теорема доказана для распределенных систем, нельзя пожертвовать `Partition tolerance`, потому что распределенная система всегда имеет дело с сетью. Максимум что можно отнести к этой категории, это базы данных, которые установлены на один физический сервер.

## Типы баз данных
Сложно отнести современные БД к одной категории. Например для Cassandra можно настроить
MySQL может быть CP или AP в зависимости от конфигурации.


## Недостатки CAP-теоремы

## Дальнейшее чтение
* [Overview of Consistency Levels in Database Systems](http://dbmsmusings.blogspot.com/2019/07/overview-of-consistency-levels-in.html)
* [Consistency model Wikipedia](https://en.wikipedia.org/wiki/Consistency_model)
* [Big Data World, Part 5: CAP Theorem](https://blog.jetbrains.com/blog/2021/06/03/big-data-world-part-5-cap-theorem/)
* [Мифы о CAP теореме](https://habr.com/ru/post/322276/)
* [Please stop calling databases CP or AP](https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html) ([Перевод на русский](https://habr.com/ru/post/258145/))
